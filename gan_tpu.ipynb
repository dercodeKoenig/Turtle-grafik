{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan-tpu",
      "provenance": [],
      "mount_file_id": "1bDVn6UjH_MvUtnC0AApIxaIUy8opqpgg",
      "authorship_tag": "ABX9TyNs3TVvhEnIufvrBkS3s3Tb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dercodeKoenig/Turtle-grafik/blob/main/gan_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAejSaiJ94qN",
        "outputId": "c5b1b73c-7b0f-4d74-ff5e-abf4f2764c4a"
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import cv2\n",
        "from IPython import display\n",
        "\n",
        "t0= time.time()\n",
        "\n",
        "train_images_cartoon = np.load(\"npfilet.npy\")\n",
        "train_images_cartoon = tf.keras.layers.experimental.preprocessing.RandomZoom((0,0.4),fill_mode='constant',fill_value=1)(train_images_cartoon)\n",
        "train_images_cartoon = tf.keras.layers.experimental.preprocessing.RandomTranslation((-0.2,0.2), (-0.2,0.2),fill_mode='constant',fill_value=1)(train_images_cartoon)\n",
        "\n",
        "train_images_cartoon1=train_images_cartoon[0:int(1*train_images_cartoon.shape[0]/4)]\n",
        "train_images_cartoon2=train_images_cartoon[int(1*train_images_cartoon.shape[0]/4):int(2*train_images_cartoon.shape[0]/4)]\n",
        "train_images_cartoon3=train_images_cartoon[int(2*train_images_cartoon.shape[0]/4):int(3*train_images_cartoon.shape[0]/4)]\n",
        "train_images_cartoon4=train_images_cartoon[int(3*train_images_cartoon.shape[0]/4):int(4*train_images_cartoon.shape[0]/4)]\n",
        "del train_images_cartoon\n",
        "\n",
        "small_images_cartoon = np.load(\"npfiles.npy\")\n",
        "small_images_cartoon = tf.keras.layers.experimental.preprocessing.RandomZoom((0,0.4),fill_mode='constant',fill_value=1)(small_images_cartoon)\n",
        "small_images_cartoon = tf.keras.layers.experimental.preprocessing.RandomTranslation((-0.2,0.2), (-0.2,0.2),fill_mode='constant',fill_value=1)(small_images_cartoon)\n",
        "test_imgs = small_images_cartoon[0:20]\n",
        "\n",
        "small_images_cartoon1=small_images_cartoon[0:int(1*small_images_cartoon.shape[0]/4)]\n",
        "small_images_cartoon2=small_images_cartoon[int(1*small_images_cartoon.shape[0]/4):int(2*small_images_cartoon.shape[0]/4)]\n",
        "small_images_cartoon3=small_images_cartoon[int(2*small_images_cartoon.shape[0]/4):int(3*small_images_cartoon.shape[0]/4)]\n",
        "small_images_cartoon4=small_images_cartoon[int(3*small_images_cartoon.shape[0]/4):int(4*small_images_cartoon.shape[0]/4)]\n",
        "del small_images_cartoon\n",
        "\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 6\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * 8 #8TPUs\n",
        "\n",
        "try:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "except:\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "train_images_cartoon1 = tf.data.Dataset.from_tensor_slices((train_images_cartoon1)).batch(GLOBAL_BATCH_SIZE)  \n",
        "train_images_cartoon2 = tf.data.Dataset.from_tensor_slices((train_images_cartoon2)).batch(GLOBAL_BATCH_SIZE) \n",
        "train_images_cartoon3 = tf.data.Dataset.from_tensor_slices((train_images_cartoon3)).batch(GLOBAL_BATCH_SIZE) \n",
        "train_images_cartoon4 = tf.data.Dataset.from_tensor_slices((train_images_cartoon4)).batch(GLOBAL_BATCH_SIZE) \n",
        "train_images_cartoon1 = strategy.experimental_distribute_dataset(train_images_cartoon1)\n",
        "train_images_cartoon2 = strategy.experimental_distribute_dataset(train_images_cartoon2)\n",
        "train_images_cartoon3 = strategy.experimental_distribute_dataset(train_images_cartoon3)\n",
        "train_images_cartoon4 = strategy.experimental_distribute_dataset(train_images_cartoon4)\n",
        "\n",
        "small_images_cartoon1 = tf.data.Dataset.from_tensor_slices((small_images_cartoon1)).batch(GLOBAL_BATCH_SIZE)  \n",
        "small_images_cartoon2 = tf.data.Dataset.from_tensor_slices((small_images_cartoon2)).batch(GLOBAL_BATCH_SIZE) \n",
        "small_images_cartoon3 = tf.data.Dataset.from_tensor_slices((small_images_cartoon3)).batch(GLOBAL_BATCH_SIZE) \n",
        "small_images_cartoon4 = tf.data.Dataset.from_tensor_slices((small_images_cartoon4)).batch(GLOBAL_BATCH_SIZE) \n",
        "small_images_cartoon1 = strategy.experimental_distribute_dataset(small_images_cartoon1)\n",
        "small_images_cartoon2 = strategy.experimental_distribute_dataset(small_images_cartoon2)\n",
        "small_images_cartoon3 = strategy.experimental_distribute_dataset(small_images_cartoon3)\n",
        "small_images_cartoon4 = strategy.experimental_distribute_dataset(small_images_cartoon4)\n",
        "\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(64,64,3)))\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(64, 5))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(32, 5))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(8*8*32, use_bias=False))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Reshape((8,8,32)))\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1),padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2),padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5),padding=\"same\", strides=(2, 2), use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    \n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5),padding=\"same\", strides=(2, 2), use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3,5,padding=\"same\",strides=(1, 1), activation='tanh', use_bias=False))\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Input(shape=(64,64,3)))\n",
        "\n",
        "    model.add(layers.Conv2D(16, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(32, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    \n",
        "    model.add(layers.Conv2D(256, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (3, 3), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    \n",
        "    model.add(layers.Conv2D(256, (3, 3), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (3, 3), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "\n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "    discriminator_cartoon = make_discriminator_model()\n",
        "    generator = make_generator_model()\n",
        "\n",
        "with strategy.scope():\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False,reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def discriminator_cartoon_loss(real_output, fake_output):\n",
        "        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "        total_loss = real_loss + fake_loss\n",
        "        return  tf.nn.compute_average_loss(total_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "\n",
        "    def _gaussian_kernel(kernel_size, sigma, n_channels, dtype):\n",
        "        x = tf.range(-kernel_size // 2 + 1, kernel_size // 2 + 1, dtype=dtype)\n",
        "        g = tf.math.exp(-(tf.pow(x, 2) / (2 * tf.pow(tf.cast(sigma, dtype), 2))))\n",
        "        g_norm2d = tf.pow(tf.reduce_sum(g), 2)\n",
        "        g_kernel = tf.tensordot(g, g, axes=0) / g_norm2d\n",
        "        g_kernel = tf.expand_dims(g_kernel, axis=-1)\n",
        "        return tf.expand_dims(tf.tile(g_kernel, (1, 1, n_channels)), axis=-1)\n",
        "\n",
        "\n",
        "    def apply_blur(img):\n",
        "        blur = _gaussian_kernel(17, 30, 3, img.dtype)\n",
        "        img = tf.nn.depthwise_conv2d(img, blur, [1,1,1,1], 'SAME')\n",
        "        return img\n",
        "\n",
        "\n",
        "    def generator_loss(fake_output,norm,genr):\n",
        "        norm=apply_blur(norm)\n",
        "        genr=apply_blur(genr)\n",
        "        total_loss=tf.nn.compute_average_loss(((norm-genr)*(norm-genr)))*0.3+0.7*tf.nn.compute_average_loss((cross_entropy(tf.ones_like(fake_output), fake_output)))\n",
        "        return  (total_loss)\n",
        "\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(0.000016)\n",
        "    discriminator_cartoon_optimizer = tf.keras.optimizers.Adam(0.00002)\n",
        "\n",
        "  \n",
        "    def train_step_cartoon(cartoons,dataset_seed):\n",
        "\n",
        "          with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_cartoon_tape :\n",
        "            \n",
        "            generated_images = generator(dataset_seed, training=True)\n",
        "\n",
        "            real_output_cartoon = discriminator_cartoon(cartoons, training=True)\n",
        "\n",
        "            fake_output_cartoon = discriminator_cartoon(generated_images, training=True)\n",
        "\n",
        "            gen_loss = generator_loss(fake_output_cartoon,dataset_seed,generated_images)\n",
        "        \n",
        "            disc_cartoon_loss = discriminator_cartoon_loss(real_output_cartoon, fake_output_cartoon)\n",
        "      \n",
        "          gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "          gradients_of_discriminator_cartoon = disc_cartoon_tape.gradient(disc_cartoon_loss, discriminator_cartoon.trainable_variables)\n",
        "      \n",
        "          generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "      \n",
        "          discriminator_cartoon_optimizer.apply_gradients(zip(gradients_of_discriminator_cartoon, discriminator_cartoon.trainable_variables))\n",
        "      \n",
        "        \n",
        "  \n",
        "@tf.function\n",
        "def train(dataset_seed,dataset_cartoon):\n",
        "    itt = iter(dataset_cartoon)\n",
        "    for seed_batch in dataset_seed:\n",
        "        strategy.run(train_step_cartoon,args=(itt.get_next(),seed_batch,))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "resume = True\n",
        "if(resume):\n",
        "    generator.load_weights('gen_weights.h5')\n",
        "    discriminator_cartoon.load_weights('disc_weights.h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.119.126.218:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.119.126.218:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFq8L3nP_Cvu"
      },
      "source": [
        "def generate_images():\n",
        "  display.clear_output(wait=True)\n",
        "  predictions = model(test_imgs,training=False)\n",
        "  generated_images = np.array(predictions,dtype= 'float32')\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 10))\n",
        "\n",
        "  for i in range(generated_images.shape[0]):\n",
        "    img =cv2.cvtColor(generated_images[i],cv2.COLOR_BGR2RGB)\n",
        "    img2 =cv2.cvtColor(test_imgs[i],cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(4, 10, i+1)\n",
        "    plt.imshow(img2)\n",
        "\n",
        "    plt.subplot(4, 10, i+2)\n",
        "    plt.imshow(img)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "while(True):\n",
        "    generate_images(generator,test_img)    \n",
        "\n",
        "    train(small_images_cartoon2,train_images_cartoon2)\n",
        "    print(\"1\")\n",
        "    train(small_images_cartoon4,train_images_cartoon4)\n",
        "    print(\"2\")\n",
        "    train(small_images_cartoon3,train_images_cartoon3)  \n",
        "    print(\"3\")\n",
        "    train(small_images_cartoon1,train_images_cartoon1)\n",
        "    print(\"4\")\n",
        "    \n",
        "    train(small_images_cartoon2,train_images_cartoon1)\n",
        "    print(\"5\")\n",
        "    train(small_images_cartoon4,train_images_cartoon2)\n",
        "    print(\"6\")\n",
        "    train(small_images_cartoon3,train_images_cartoon4)  \n",
        "    print(\"7\")\n",
        "    train(small_images_cartoon1,train_images_cartoon3)\n",
        "    print(\"8\")\n",
        "    \n",
        "    train(small_images_cartoon2,train_images_cartoon3)\n",
        "    print(\"9\")\n",
        "    train(small_images_cartoon4,train_images_cartoon1)\n",
        "    print(\"10\")\n",
        "    train(small_images_cartoon3,train_images_cartoon2)  \n",
        "    print(\"11\")\n",
        "    train(small_images_cartoon1,train_images_cartoon4)\n",
        "    print(\"12\")\n",
        "    \n",
        "    train(small_images_cartoon2,train_images_cartoon4)\n",
        "    print(\"13\")\n",
        "    train(small_images_cartoon4,train_images_cartoon3)\n",
        "    print(\"14\")\n",
        "    train(small_images_cartoon3,train_images_cartoon1)  \n",
        "    print(\"15\")\n",
        "    train(small_images_cartoon1,train_images_cartoon2)\n",
        "    print(\"16\")\n",
        "    \n",
        "    print(\"save chkp\")\n",
        "    generator.save_weights('gen_weights.h5', overwrite=True)\n",
        "    discriminator_cartoon.save_weights('disc_weights.h5', overwrite=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}